# 常见问题解答（FAQ）

### Q1：如何让赤兔支持我们的模型？
如果您是大模型开发者，希望让赤兔支持您的模型，您可以直接提交相应的 Pull Request，项目团队将在与您确认后进行合入，可参考[CONTRIBUTING](/docs/zh/CONTRIBUTING.md)。

如果您对于自行接入模型感到困难，也可以联系我们的服务支持团队 solution@chitu.ai。

### Q2：如何让赤兔支持我们的芯片？
如果您正在研制或使用某种架构的芯片，而赤兔暂未对其支持，您可以直接提交相应的 Pull Request，项目团队将在与您确认后进行合入，可参考[CONTRIBUTING](/docs/zh/CONTRIBUTING.md)。

如果您对于自行适配新的芯片架构感到困难，也可以联系我们的服务支持团队 solution@chitu.ai。

### Q3：如果芯片本身没有FP4/FP8运算单元，要怎样直接运行FP4/FP8模型呢？
可以使用FP8格式存储权重，使用BF16格式执行运算，相当于某种w8a16，但这里的8是float8。
但由于浮点数转换操作比整数转换稍微复杂一些，所以这里面会遇到一点技术挑战。
知乎上的[这篇回答](https://www.zhihu.com/question/14928372981/answer/124606559367?utm_psn=1884175276604384926 )详细解释了格式转换中的一些关键优化点。

### Q4: FP4/FP8比BF16省算力容易理解，但为什么还会有加速？
简单来说，节省一半算力的同时还能有几倍性能加速是**比较特殊的情况**，更多时候，赤兔方案带来的是性价比而非绝对性能的提升。
关于什么时候会出现这种**比较特殊的情况**，知乎上的[这篇回答](https://www.zhihu.com/question/14928372981/answer/124606559367?utm_psn=1884175276604384926 )做了一些说明。

### Q5: 赤兔和vllm、sglang、llama.cpp等开源项目的定位有何区别？
赤兔项目并非重复造轮子，其专注于多元化国产算力支持以及用户从超小规模到大规模的平滑扩展需求，是对大模型开源生态的有益补充。

### Q6: 哪些场景适合用赤兔
如果您符合以下情况之一，我们建议您尝试赤兔：
1. 使用国产算力，例如华为昇腾、沐曦、海光等；
1. 混合使用多种算力芯片；
1. 对大模型推理部署的性能指标有较高需求；
1. 希望降低大模型推理部署的算力成本；
1. 从事推理引擎研究工作。

### Q7: 赤兔支持纯CPU推理或者CPU+GPU推理么 
从 chitu v0.2.2 版本开始，支持 CPU+GPU 异构推理。纯CPU推理支持在计划中。


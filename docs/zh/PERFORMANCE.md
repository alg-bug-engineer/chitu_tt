# 性能测试数据

>  性能数据与您的硬件配置、软件版本、测试负载相关，多次测试结果可能存在波动。

如无特殊说明，下文的测试数据对应的配置为：
* 输入输出长度 input_len = 128 tokens, output_len = 1024 tokens。
* 数据类型为原版模型官方数据类型。
* 输出速率 TPS = output_tokens / (prefill_sec + decode_sec)。
* N/A 表示暂无测试数据，并非一定跑不了，也可能是没测。

## 稠密模型
### Qwen3-32B

| BS & TPS | 1xH20(96GB) | 2x910B2(64GB) | 4xDCU(64GB) |
| ---------- | ---------------- | ----- | ----- |
| 1          | 44.79 | 24.39 | 25.04 |
| 2          | 85.69 | 45.57 | 47.73 |
| 4          | 167.76 | 84.54 | 94.50 |
| 8          | 319.07 | 146.98 | 181.96 |
| 16         | 585.13 | 265.45 | 346.90 |
| 32         | 977.31 | 470.50 | 592.70 |
| 64         | 1333.49 | 805.26 | 962.24 |
| 128        | N/A | 1223.77 | N/A |

## 混合专家模型
### DeepSeek-R1-671B
| BS & TPS | 16xH20(96GB) TP8PP2 | 32xH20(96GB) DP32EP32 |
| ---------- | ---------------- | ---- |
| 1          | 57.74 | |
| 2          | 107.76 | |
| 4          | 181.68 | |
| 8          | 264.74 | |
| 16         | 420.13 | |
| 32         | 636.03 | 660.34 |
| 64         | 978.04 |  |
| 128        | 1862.98 | 2374.99 |
| 256        | | |
| 512        | | 7458.43 |
| 1024       | | 11696.37 |
| 2048       | | 16022.60 |

### Kimi-K2-1T
| BS & TPS | 16xH20(96GB) |
| ---------- | ---------------- |
| 1          | 47.51 |
| 2          | 92.02 |
| 4          | 163.94 |
| 8          | 241.52 |
| 16         | 403.08 |
| 32         | 634.12 |
| 64         | 943.05 |
| 128        | 1383.46 |
| 256        | 2571.74 |

### GLM-4.5-Air-106B-A12B
| BS & TPS | 8xH20(96GB) | 8x910B2(64GB) |
| ---------- | ---------------- | --- |
| 1          | 113.28           | 30.81 |
| 2          | 193.84           | 54.51 |
| 4          | 352.84           | 98.48 |
| 8          | 621.75           | 168.50 |
| 16         | 1058.10          | 286.75 |
| 32         | 1774.08          | 477.89 |
| 64         | 2986.52          | 796.72 |
| 128        | 4757.48          | 1317.03 |

## 量化模型
### 在单机八卡 H20(96G) 服务器上部署 DeepSeek-R1-671B

| 输出速率 token/s| chitu 0.3.0, 原版 FP8| chitu 0.3.0, FP4->FP8 | chitu 0.3.0, FP4->BF16 |
|:---|:---|:---|:---|
|bs=1| 24.30 | 20.70 | 19.78 |
|bs=16| 203.71 | 89.56 | 110.68 |
|bs=64| OOM | 237.20 | 232.14 |
|bs=128| OOM | 360.80 | 351.73 |
| **MMLU 得分** | 89.8 | 88.0 | 88.0 |

- 八卡机的显存总容量为768GB，而原版模型的权重需要接近700GB，因此可支持的并发数不大。
- FP4 量化版模型的权重仅需不到400GB的显存空间，因此可支持更大的并发数；也使得 GPU 配置为 8*64GB 的服务器可以轻松部署 671B 模型。
- 上表的性能测试使用的输入和输出长度均为 512 tokens。
- 在 MMLU 精度测试中，FP4 量化版得分 (88.0) 优于 INT8 量化版 (87.2) 和 INT4 量化版 (82.1) ，比原版降低约 2%。

### 在 Xeon 8480P + H20(96G) 服务器上异构部署 DeepSeek-R1-671B

| 完整放置于 GPU 的层数       | GPU 卡数 | output token/s (bs=1) | output token/s (bs=16) |
|:---------------|:------|:---------------|:----------------|
| 0    | 1    | 10.61          | 28.16            |
| 24    | 2    | 14.04           | 42.57        |

- 使用的模型是 DeepSeek-R1-671B 的 Q4 量化版本（INT4）。
- 测试数据基于 Chitu v0.2.2 版本。
- 性能瓶颈在 CPU 一侧，增加 GPU 数量后性能提升有限，建议采用更高端的 CPU 和主存。
- 适用于 GPU 显存受限且不需要高并发支持的场景。
- MMLU 测试得分约 83。

### 在 A800(40GB) 集群上部署 DeepSeek-R1-671B

|BS & TPS |6 节点, BF16 |3 节点, FP8|
|:---|:---|:---|
|1| 29.8 | 22.7 |
|4| 78.8 | 70.1 |
|8| 129.8 | 108.9 |
|16| 181.4 | 159.0 |
|32| 244.1 | 214.5 |

- 测试版本为 chitu v0.1.0，性能可能显著低于最新版本。
- 从不同batchsize的测试数据来看，同样基于Chitu引擎，使用3节点运行FP8模型的输出速度约为使用6节点运行BF16模型的75%\~90%，即单位算力的产出获得了1.5x\~1.8x的提升。
- 这是由于解码Decoding过程主要依赖于访存带宽，使用一半的GPU去访问一半的数据（FP8的权重大小是BF16的一半）不会消耗更长的时间，GPU计算能力缩减只带来较小的影响。

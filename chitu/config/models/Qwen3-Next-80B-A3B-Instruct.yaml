# SPDX-FileCopyrightText: 2025 Qingcheng.AI
#
# SPDX-License-Identifier: Apache-2.0

name: Qwen3-Next-80B-A3B-Instruct
type: hf-qwen3-next
source: "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct" # Just for displaying. No automatic download.
ckpt_dir: null # Please override this in commond line with `models.ckpt_dir=<path>`.
tokenizer_path: null # By default the same as ckpt_dir, or you can override it with `models.tokenizer_path=<path>`.
tokenizer_type: "hf" # one of: hf, tiktoken
dim: 2048
n_layers: 48
n_heads: 16
n_kv_heads: 2
vocab_size: 151936
moe_intermediate_dim: 512
num_experts: 512
num_experts_per_tok: 10
norm_topk_prob: True
norm_eps: 1e-06
rope_theta: 10000000.0
qkv_has_bias: False
head_dim: 256
use_qk_norm: True
linear_conv_kernel_dim: 4
linear_head_dim: 128
linear_n_qk_heads: 16
linear_n_v_heads: 32
bos_token_id: 151643
eos_token_id: 151645
full_attention_interval: 4
num_full_attention_blocks: -1 # kv cache blocks for full attention layers, could override num_blocks
num_linear_attention_blocks: -1 # kv cache blocks for linear attention layers, could override num_blocks
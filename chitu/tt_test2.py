import os
import sys
import time
import statistics
from types import SimpleNamespace
import torch

# ç¡®ä¿è·¯å¾„åŒ…å« chitu æºç 
sys.path.append("/workspace/chitu_cc")

from chitu.chitu_main import chitu_init, chitu_start, chitu_run, chitu_terminate
from chitu.backend import Backend
from chitu.task import Task, TaskPool, UserRequest
from chitu.task_type import TaskDecodeType

# --- é…ç½®æ„å»ºå‡½æ•° ---
def build_args_tt_qwen_batch(hf_model_path: str | None = None):
    models = SimpleNamespace(
        name="TT-Qwen2.5",
        type="tt-qwen",
        ckpt_dir=hf_model_path,
        tokenizer_path=hf_model_path,
        tokenizer_type="hf",
    )
    infer = SimpleNamespace(
        seed=42,
        dp_size=1,
        tp_size=1,
        pp_size=1,
        ep_size=1,
        # å…³é”®è®¾ç½®ï¼šmax_reqs å¿…é¡» >= 32 ä»¥æ»¡è¶³ TT åº•å±‚ Tile å¯¹é½è¦æ±‚
        # å³ä½¿æˆ‘ä»¬åªæµ‹ batch=8ï¼Œåº•å±‚ä¹Ÿéœ€è¦åˆ†é… 32 çš„ç©ºé—´
        max_reqs=32,  
        max_seq_len=1024, # å¢åŠ é•¿åº¦ä»¥é€‚åº”å¤šè½®æµ‹è¯•
        prefill_chunk_size=None,
        bind_process_to_cpu="none",
        use_cuda_graph="auto",
        attn_type="auto",
        op_impl="tt",
        mla_absorb="none",
        cache_type="skew",
        num_blocks=1,
        memory_utilization=0.98,
        raise_lower_bit_float_to="bfloat16",
    )
    dp_router = SimpleNamespace(is_router=False, pd_disaggregation=SimpleNamespace(enabled=False))
    dp_config = SimpleNamespace(dp_id=0, router=dp_router, scheduler_base_port=5557, scheduler_base_host="127.0.0.1", router_host="127.0.0.1", router_port=5556)
    
    # ä½¿ç”¨ FCFS ç®€å•è°ƒåº¦
    scheduler = SimpleNamespace(type="fcfs")
    
    pp_config = SimpleNamespace(
        prefill_num_tasks=1,
        decode_num_tasks=1,
        prefill_num_tasks_divided_by_pp=False,
        enforce_decode_num_tasks_max=False,
    )
    
    args = SimpleNamespace(
        infer=infer,
        models=models,
        dp_config=dp_config,
        scheduler=scheduler,
        pp_config=pp_config,
        float_16bit_variant="bfloat16",
        skip_preprocess=False,
        debug=SimpleNamespace(skip_model_load=False),
    )
    return args

# --- åŸºå‡†æµ‹è¯•å‡½æ•° ---
def run_benchmark(batch_size, max_new_tokens=64):
    # å‡†å¤‡æµ‹è¯• Prompt æ± 
    base_prompts = [
        "ä»‹ç»ä¸‹äººå·¥æ™ºèƒ½",
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„ä¸ƒè¨€ç»å¥",
        "Explain Quantum Computing simply",
        "1+1ç­‰äºå‡ ï¼Ÿ",
        "åŒ—äº¬æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ",
        "What is the capital of France?",
        "åˆ—ä¸¾3ä¸ªå¥åº·çš„æ°´æœ",
        "Pythonä¸­åˆ—è¡¨å’Œå…ƒç»„çš„åŒºåˆ«ï¼Ÿ"
    ]
    # å¦‚æœ batch_size å¤§äºé¢„è®¾ prompt æ•°é‡ï¼Œå¾ªç¯å¡«å……
    current_prompts = []
    while len(current_prompts) < batch_size:
        current_prompts.extend(base_prompts)
    current_prompts = current_prompts[:batch_size]
    
    print(f"\n" + "-"*60)
    print(f"ğŸš€ å¼€å§‹æµ‹è¯• Batch Size = {batch_size}")
    print(f"-"*60)
    
    tasks = []
    # æ„é€ ä»»åŠ¡
    for i, prompt in enumerate(current_prompts):
        req = UserRequest(
            message=[{"role": "user", "content": prompt}],
            request_id=f"bench-b{batch_size}-{i}",
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=1.0,
            top_k=1,
        )
        task = Task(task_id=req.request_id, req=req, stop_with_eos=True)
        TaskPool.add(task)
        tasks.append(task)

    # ç»Ÿè®¡å˜é‡
    start_time = time.perf_counter()
    first_token_times = [None] * batch_size
    completion_times = [None] * batch_size
    
    step = 0
    active_tasks = batch_size
    
    # æ¨ç†å¾ªç¯
    while active_tasks > 0:
        chitu_run()
        step += 1
        
        current_time = time.perf_counter()
        
        # æ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„çŠ¶æ€
        completed_in_this_step = 0
        for i, task in enumerate(tasks):
            # è®°å½•é¦–å­—æ—¶é—´ (TTFT)
            if first_token_times[i] is None and task.num_new_tokens > 0:
                first_token_times[i] = current_time - start_time
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if completion_times[i] is None:
                is_stopped = task._decode_status in (TaskDecodeType.Stopped, TaskDecodeType.StopEOS, TaskDecodeType.StopLength)
                is_completed = task.req.completed.is_set()
                
                if is_stopped or is_completed:
                    completion_times[i] = current_time - start_time
                    # æ‰“å°éƒ¨åˆ†ç”Ÿæˆç»“æœç”¨äºéªŒè¯
                    if hasattr(task, 'response') and len(task.response) > 0:
                        # ç®€ç•¥æ‰“å°å‰10ä¸ªtoken idè¯æ˜åœ¨å·¥ä½œ
                        token_preview = task.response.to_tensor().cpu().tolist()[:5]
                        print(f"  [Task {i}] å®Œæˆ. Tokens: {task.num_new_tokens} Preview: {token_preview}...")
                    
        # æ›´æ–°å‰©ä½™ä»»åŠ¡æ•°
        active_tasks = batch_size - sum(1 for t in completion_times if t is not None)
        
        # è¶…æ—¶ä¿æŠ¤
        if step > max_new_tokens * 2 + 50:
            print("âš ï¸ è­¦å‘Š: è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œå¼ºåˆ¶åœæ­¢")
            break

    total_time = time.perf_counter() - start_time
    
    # --- è®¡ç®—æŒ‡æ ‡ ---
    total_tokens = sum(t.num_new_tokens for t in tasks)
    
    # è¿‡æ»¤æ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    valid_completion_times = [t for t in completion_times if t is not None]
    valid_ttft_times = [t for t in first_token_times if t is not None]
    
    avg_latency = statistics.mean(valid_completion_times) if valid_completion_times else 0
    avg_ttft = statistics.mean(valid_ttft_times) if valid_ttft_times else 0
    tps = total_tokens / total_time if total_time > 0 else 0
    
    print(f"âœ… å®Œæˆ Batch={batch_size}. è€—æ—¶: {total_time:.2f}s, æ€»Tokens: {total_tokens}, TPS: {tps:.2f}")
    
    return {
        "batch_size": batch_size,
        "total_time": total_time,
        "total_tokens": total_tokens,
        "tps": tps,
        "avg_ttft": avg_ttft,
        "avg_latency": avg_latency
    }

def main():
    # 1. ç¯å¢ƒè®¾ç½®
    default_model_path = "/workspace/Qwen2.5-0.5B-Instruct"
    hf_model = os.environ.get("HF_MODEL", default_model_path)
    os.environ["HF_MODEL"] = hf_model
    print(f"Using model path: {hf_model}")

    # 2. åˆå§‹åŒ–å¼•æ“ (å•æ¬¡åˆå§‹åŒ–ï¼Œå¤šæ¬¡è¿è¡Œ)
    args = build_args_tt_qwen_batch(hf_model)
    chitu_init(args)
    chitu_start()

    # 3. Warmup (å¯é€‰ï¼Œè¿è¡Œä¸€ä¸ªå°ä»»åŠ¡é¢„çƒ­ç¼–è¯‘ç¼“å­˜)
    print("\nğŸ”¥ Pre-warming engine...")
    run_benchmark(batch_size=1, max_new_tokens=10)
    
    # 4. æ‰§è¡Œå¤š Batch æµ‹è¯•
    benchmark_results = []
    # æµ‹è¯•åˆ—è¡¨ï¼Œå¯æ ¹æ®æ˜¾å­˜å’Œè€—æ—¶æƒ…å†µè°ƒæ•´
    test_batches = [1, 2, 4, 8] 
    
    for bs in test_batches:
        # ç¨å¾®æš‚åœï¼Œç¡®ä¿ä¹‹å‰çš„ä»»åŠ¡æ¸…ç†å®Œæ¯•ï¼ˆè™½ç„¶ TaskPool é€»è¾‘åº”å¤„ç†å¥½ï¼‰
        time.sleep(1)
        res = run_benchmark(batch_size=bs, max_new_tokens=64)
        benchmark_results.append(res)
    
    # 5. è¾“å‡ºå¯¹æ¯”è¡¨æ ¼
    print("\n\n" + "="*90)
    print(f"{'Batch':<6} | {'Total Time(s)':<14} | {'Total Tokens':<12} | {'TPS (sys)':<10} | {'TTFT (s)':<10} | {'Avg Latency(s)':<14}")
    print("-" * 90)
    for r in benchmark_results:
        print(f"{r['batch_size']:<6} | {r['total_time']:<14.3f} | {r['total_tokens']:<12} | {r['tps']:<10.2f} | {r['avg_ttft']:<10.3f} | {r['avg_latency']:<14.3f}")
    print("="*90)
    
    # 6. ç»“æŸ
    chitu_terminate()
    print("\nDone.")

if __name__ == "__main__":
    main()
#!/bin/bash

# SPDX-FileCopyrightText: 2025 Qingcheng.AI
#
# SPDX-License-Identifier: Apache-2.0

# ğŸ”§ DP=n è¿ç»­åˆ†ç»„ + Router æ··åˆæ¨¡å¼æµ‹è¯•è„šæœ¬
# æ¶æ„ï¼š1ä¸ªRouter + 2ä¸ªè¿ç»­DPç»„ï¼Œæ¯ç»„1å¡ (TP=1)
# Router: æä¾›HTTPæœåŠ¡ï¼Œè´Ÿè½½å‡è¡¡åˆ°DPç»„
# DPç»„: ä½¿ç”¨torch.distributed.new_group + ç»„å†…rankç¼–å·0~1

# åœ¨srunä¹‹å‰è®¾ç½®condaç¯å¢ƒ
conda activate chitu-env-bf16

# é…ç½®å‚æ•°
MODEL_CONFIG=${1:-"Qwen3-32B"}
MODEL_CKPT_DIR=${2:-"/data/nfs/Qwen3-32B"}

DP_GROUPS=2  # åªéœ€æ”¹è¿™é‡Œå³å¯
# SLURMé…ç½®
SLURM_PARTITION=debug
CPUS_PER_GPU=24
MEM_PER_GPU=242144
GPUS_PER_GROUP=1
NUM_GPUS=$((DP_GROUPS * GPUS_PER_GROUP))
NUM_CPUS=$((NUM_GPUS * CPUS_PER_GPU))
NUM_MEMS=$((NUM_GPUS * MEM_PER_GPU))

echo "=== DP=${DP_GROUPS} è¿ç»­åˆ†ç»„ + Router æ··åˆæ¨¡å¼æµ‹è¯•å¯åŠ¨ ==="
echo "æ¨¡å‹é…ç½®: $MODEL_CONFIG"
echo "æ¨¡å‹è·¯å¾„: $MODEL_CKPT_DIR"
echo "æ¶æ„: 1ä¸ªRouter + ${DP_GROUPS}ä¸ªè¿ç»­DPç»„"

# ğŸ”§ è®¾ç½®å…³é”®ç¯å¢ƒå˜é‡
export CHITU_USE_CONTIGUOUS_DP_GROUPS=1
export CUDA_LAUNCH_BLOCKING=1
export DP_GROUPS=2
export GPUS_PER_GROUP=1

echo "=== å¯åŠ¨å‚æ•° ==="
echo "GPUæ•°é‡: $NUM_GPUS"
echo "CPUæ•°é‡: $NUM_CPUS"
echo "å†…å­˜: ${NUM_MEMS}MB"
echo "ç¯å¢ƒå˜é‡: CHITU_USE_CONTIGUOUS_DP_GROUPS=$CHITU_USE_CONTIGUOUS_DP_GROUPS"

srun --partition=${SLURM_PARTITION} \
     --gres=gpu:${NUM_GPUS} \
     --cpus-per-task=${NUM_CPUS} \
     --mem=${NUM_MEMS}MB \
     --nodes=1 \
     --ntasks=1 \
     --job-name=dp_2_contiguous_router \
     --time=01:00:00 \
     bash -c "
        set -e
        export CHITU_USE_CONTIGUOUS_DP_GROUPS=1
        export CUDA_LAUNCH_BLOCKING=1
        export DP_GROUPS=2 GPUS_PER_GROUP=1

        # æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
        echo '=== ç¯å¢ƒä¿¡æ¯ ==='
        echo \"CUDA_VISIBLE_DEVICES: \$CUDA_VISIBLE_DEVICES\"
        echo \"SLURM_PROCID: \$SLURM_PROCID\"
        echo \"SLURM_LOCALID: \$SLURM_LOCALID\"
        echo \"CHITU_USE_CONTIGUOUS_DP_GROUPS: \$CHITU_USE_CONTIGUOUS_DP_GROUPS\"
        nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv
        echo \"å¯ç”¨GPUæ•°é‡: \$(nvidia-smi -L | wc -l)\"
        echo \"Torchå¯è§GPUæ•°é‡: \$(python -c 'import torch; print(torch.cuda.device_count())')\"

        
        
        # ğŸ”§ æ­¥éª¤1ï¼šå¯åŠ¨ç‹¬ç«‹Routerè¿›ç¨‹
        echo '=== æ­¥éª¤1ï¼šå¯åŠ¨ç‹¬ç«‹Routerè¿›ç¨‹ ==='
        echo 'ç‰¹æ€§ï¼šæä¾›HTTPæœåŠ¡ï¼Œè´Ÿè½½å‡è¡¡åˆ°DPç»„'
        CHITU_INDEPENDENT_ROUTER=1 CHITU_ROUTER_PROCESS=1 python -m chitu \
                 models=${MODEL_CONFIG} \
                 models.ckpt_dir=${MODEL_CKPT_DIR} \
                 dp_config.router.host=0.0.0.0 \
                 dp_config.router.port=21003 \
                 dp_config.enabled=True \
                 dp_config.dp_size=2 \
                 dp_config.router.stats_port=29600 \
                 dp_config.router.token_port=29700 \
                 dp_config.router.is_router=True &
        
        ROUTER_PID=\$!
        echo \"âœ… Routerè¿›ç¨‹å¯åŠ¨ï¼ŒPID: \$ROUTER_PIDï¼ŒHTTPç«¯å£: 21003, ZMQç«¯å£: 29600(stats), 29700(token)\"
        sleep 30
        
        # æ£€æŸ¥Routerè¿›ç¨‹
        if kill -0 \$ROUTER_PID 2>/dev/null; then
            echo \"âœ… Routerè¿›ç¨‹æ­£å¸¸è¿è¡Œ\"
        else
            echo \"âŒ Routerè¿›ç¨‹å¯åŠ¨å¤±è´¥\"
            exit 1
        fi
        
        # æ£€æŸ¥ç«¯å£
        if nc -z localhost 29600 && nc -z localhost 29700 && nc -z localhost 21003; then
            echo \"âœ… Routerç«¯å£æ£€æŸ¥é€šè¿‡\"
        else
            echo \"âŒ Routerç«¯å£æ£€æŸ¥å¤±è´¥\"
            exit 1
        fi

        DP_GROUP_PIDS=()
        for ((i=0; i<$DP_GROUPS; i++)); do
            GPU_START=\$((i * 1)) 
            GPU_END=\$((GPU_START + 1 - 1))
            GPUS=\$(seq -s, \$GPU_START \$GPU_END) 
            MASTER_PORT=\$((29502 + i))
            SCHEDULER_PORT=\$((29610 + i))
            echo \" -- GPU_START ä¸º: \$GPU_START -- \"
            echo \" -- GPU_END ä¸º: \$GPU_END -- \"
            echo \" -- GPUS ä¸º: \$GPUS -- \"
            echo \" -- MASTER_PORT ä¸º: \$MASTER_PORT -- \"
            echo \" -- SCHEDULER_PORT ä¸º: \$SCHEDULER_PORT -- \"

            if (( i < DP_GROUPS - 1 )); then
                echo \"=== å¯åŠ¨ç¬¬ \$((i+1))ä¸ªDPç»„ ===\"
                CUDA_VISIBLE_DEVICES=\$GPUS torchrun --nproc_per_node=1 \
                    --master_port=\$MASTER_PORT \
                    -m chitu \
                    models='"${MODEL_CONFIG}"' \
                    models.ckpt_dir='"${MODEL_CKPT_DIR}"' \
                    infer.tp_size=1 \
                    infer.pp_size=1 \
                    infer.cache_type=paged \
                    infer.max_seq_len=2048 \
                    infer.max_reqs=128 \
                    request.max_new_tokens=1200 \
                    dp_config.enabled=True \
                    dp_config.dp_id=\$i \
                    dp_config.scheduler_base_host=0.0.0.0 \
                    dp_config.scheduler_base_port=\$SCHEDULER_PORT \
                    infer.use_cuda_graph=false &
                DP_GROUP_PIDS+=($!)
                sleep 30
            else
                echo \"=== å¯åŠ¨ç¬¬ \$((i+1))ä¸ªDPç»„ ===\"
                CUDA_VISIBLE_DEVICES=\$GPUS torchrun --nproc_per_node=1 \
                    --master_port=\$MASTER_PORT \
                    -m chitu \
                    models='"${MODEL_CONFIG}"' \
                    models.ckpt_dir='"${MODEL_CKPT_DIR}"' \
                    infer.tp_size=1 \
                    infer.pp_size=1 \
                    infer.cache_type=paged \
                    infer.max_seq_len=2048 \
                    infer.max_reqs=128 \
                    request.max_new_tokens=1200 \
                    dp_config.enabled=True \
                    dp_config.dp_id=\$i \
                    dp_config.scheduler_base_host=0.0.0.0 \
                    dp_config.scheduler_base_port=\$SCHEDULER_PORT \
                    infer.use_cuda_graph=false
            fi
        done

        # ç­‰å¾…æ‰€æœ‰åå°DPç»„
        for pid in \"${DP_GROUP_PIDS[@]}\"; do
            wait $pid || { echo \"å­è¿›ç¨‹$pidå¤±è´¥\"; exit 1; }
        done
        wait $ROUTER_PID
     "

echo ""
echo "=== æµ‹è¯•å‘½ä»¤ ==="
echo "curl -X POST http://localhost:21003/v1/chat/completions \\"
echo "  -H 'Content-Type: application/json' \\"
echo "  -d '{\"messages\":[{\"role\":\"user\",\"content\":\"Introduce yourself!\"}],\"max_tokens\":50}'" 
